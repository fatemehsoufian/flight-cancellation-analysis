{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6533c323-ddd2-4467-96f1-93141add03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataProject\").getOrCreate()\n",
    "\n",
    "all_flights = spark.read.csv(\"all_flights.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d61d97-f744-44b3-91a8-f494f7086441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 61556964\n"
     ]
    }
   ],
   "source": [
    "row_count = all_flights.count()\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9548dc12-bedf-42d8-af8b-d0586a7a8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flights = all_flights.drop('Unnamed: 27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48df863-6019-4c51-9e48-0fa22a16b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled flights rows: 973209\n",
      "Number of normal flights rows: 60583755\n"
     ]
    }
   ],
   "source": [
    "canceled_flights = all_flights.filter(all_flights.CANCELLED == 1)\n",
    "normal_flights = all_flights.filter(all_flights.CANCELLED == 0)\n",
    "\n",
    "canceled_flights_row_count = canceled_flights.count()\n",
    "normal_flights_row_count = normal_flights.count()\n",
    "\n",
    "print(f\"Number of canceled flights rows: {canceled_flights_row_count}\")\n",
    "print(f\"Number of normal flights rows: {normal_flights_row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef423d82-e6a5-406e-84a2-5e68254f1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to fill\n",
    "columns_to_fill = [\"WHEELS_ON\", \"TAXI_IN\", \"ARR_TIME\", \"ARR_DELAY\", \"ACTUAL_ELAPSED_TIME\", \"AIR_TIME\"]\n",
    "\n",
    "# Fill all specified columns with -1 where they are null\n",
    "canceled_flights = canceled_flights.fillna({col: -1 for col in columns_to_fill})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e547265-b058-4df1-bcfd-af869a2eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# plane left the gate (maybe even moved but never took off fully)\n",
    "weird_canceled_flights =canceled_flights.filter((col(\"DEP_TIME\").isNotNull()))\n",
    "other_canceled_flights =canceled_flights.filter((col(\"DEP_TIME\").isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df82b97-961d-4531-a0e9-568745c0ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_flights = normal_flights.filter(\n",
    "    (col(\"DEP_DELAY\") > 0)\n",
    ")\n",
    "\n",
    "# Flights not delayed (DEP_DELAY <= 0)\n",
    "non_delayed_flights = normal_flights.filter(\n",
    "    (col(\"DEP_DELAY\") <= 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba172188-4d88-4197-b1ad-bf492e84c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in weird_canceled_flights: 37486\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 208 nulls\n",
      "TAXI_OUT: 28178 nulls\n",
      "WHEELS_OFF: 28173 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 26 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 37486 nulls\n",
      "WEATHER_DELAY: 37486 nulls\n",
      "NAS_DELAY: 37486 nulls\n",
      "SECURITY_DELAY: 37486 nulls\n",
      "LATE_AIRCRAFT_DELAY: 37486 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = weird_canceled_flights.count()\n",
    "\n",
    "print(\"Number of rows in weird_canceled_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in weird_canceled_flights.columns:\n",
    "    null_count = weird_canceled_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755ad57b-4491-4574-915e-afda1d9a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling dep delay missing values in weird canceled flights\n",
    "\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Step 1: Convert CRS_DEP_TIME to total minutes (temporary columns)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_HOUR\", floor(col(\"CRS_DEP_TIME\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_MIN\", col(\"CRS_DEP_TIME\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_TOTAL_MIN\", (col(\"CRS_DEP_HOUR\") * 60) + col(\"CRS_DEP_MIN\"))\n",
    "\n",
    "# Step 2: Convert DEP_TIME to total minutes (temporary columns)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_HOUR\", floor(col(\"DEP_TIME\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_MIN\", col(\"DEP_TIME\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_TOTAL_MIN\", (col(\"DEP_HOUR\") * 60) + col(\"DEP_MIN\"))\n",
    "\n",
    "# Step 3: Calculate DEP_DELAY\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\n",
    "    \"DEP_DELAY\",\n",
    "    when(\n",
    "        col(\"DEP_DELAY\").isNull(),\n",
    "        col(\"DEP_TOTAL_MIN\") - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "    ).otherwise(\n",
    "        col(\"DEP_DELAY\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 4: Drop helper columns to clean\n",
    "weird_canceled_flights = weird_canceled_flights.drop(\n",
    "    \"CRS_DEP_HOUR\", \"CRS_DEP_MIN\", \"CRS_DEP_TOTAL_MIN\",\n",
    "    \"DEP_HOUR\", \"DEP_MIN\", \"DEP_TOTAL_MIN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8c1b22-4e2c-41cb-bda8-572913744d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling wheels off missing values in weird canceled flights\n",
    "weird_canceled_flights = weird_canceled_flights.fillna({\"WHEELS_OFF\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916180ca-3241-4677-9bc4-4747b6353bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling taxi out missing values in weird canceled flights\n",
    "from pyspark.sql.functions import floor, col, when\n",
    "\n",
    "# Step 1: Convert DEP_TIME and WHEELS_OFF into total minutes\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_HOUR\", floor(col(\"DEP_TIME\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_MIN\", col(\"DEP_TIME\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"DEP_TOTAL_MIN\", col(\"DEP_HOUR\") * 60 + col(\"DEP_MIN\"))\n",
    "\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"WHEELS_OFF_HOUR\", floor(col(\"WHEELS_OFF\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"WHEELS_OFF_MIN\", col(\"WHEELS_OFF\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"WHEELS_OFF_TOTAL_MIN\", col(\"WHEELS_OFF_HOUR\") * 60 + col(\"WHEELS_OFF_MIN\"))\n",
    "\n",
    "# Step 2: Handle missing or calculate TAXI_OUT\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\n",
    "    \"TAXI_OUT\",\n",
    "    when(\n",
    "        col(\"TAXI_OUT\").isNull(),\n",
    "        when(\n",
    "            (col(\"WHEELS_OFF\") == -1) | (col(\"DEP_TIME\").isNull()) | (col(\"WHEELS_OFF\").isNull()),\n",
    "            -1\n",
    "        ).otherwise(\n",
    "            col(\"WHEELS_OFF_TOTAL_MIN\") - col(\"DEP_TOTAL_MIN\")\n",
    "        )\n",
    "    ).otherwise(\n",
    "        col(\"TAXI_OUT\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: (optional) Drop helper columns to keep DataFrame clean\n",
    "weird_canceled_flights = weird_canceled_flights.drop(\"DEP_HOUR\", \"DEP_MIN\", \"DEP_TOTAL_MIN\", \"WHEELS_OFF_HOUR\", \"WHEELS_OFF_MIN\", \"WHEELS_OFF_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df51109-6c77-4975-a04d-d5b2ffa93f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling crs elapsed time missing values in weird canceled flights\n",
    "from pyspark.sql.functions import floor, col, when\n",
    "\n",
    "# Step 1: Convert CRS_DEP_TIME and CRS_ARR_TIME to total minutes\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_HOUR\", floor(col(\"CRS_DEP_TIME\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_MIN\", col(\"CRS_DEP_TIME\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_DEP_TOTAL_MIN\", col(\"CRS_DEP_HOUR\") * 60 + col(\"CRS_DEP_MIN\"))\n",
    "\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_ARR_HOUR\", floor(col(\"CRS_ARR_TIME\") / 100))\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_ARR_MIN\", col(\"CRS_ARR_TIME\") % 100)\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\"CRS_ARR_TOTAL_MIN\", col(\"CRS_ARR_HOUR\") * 60 + col(\"CRS_ARR_MIN\"))\n",
    "\n",
    "# Step 2: Calculate CRS_ELAPSED_TIME\n",
    "weird_canceled_flights = weird_canceled_flights.withColumn(\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"CRS_ELAPSED_TIME\").isNull(),\n",
    "        when(\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") >= col(\"CRS_DEP_TOTAL_MIN\"),\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"CRS_ARR_TOTAL_MIN\") + 1440) - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        )\n",
    "    ).otherwise(\n",
    "        col(\"CRS_ELAPSED_TIME\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "weird_canceled_flights = weird_canceled_flights.drop(\"CRS_DEP_HOUR\", \"CRS_DEP_MIN\", \"CRS_DEP_TOTAL_MIN\", \"CRS_ARR_HOUR\", \"CRS_ARR_MIN\", \"CRS_ARR_TOTAL_MIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d29534-6c27-4e13-827f-0a78feac9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = [\n",
    "    \"LATE_AIRCRAFT_DELAY\",\n",
    "    \"SECURITY_DELAY\",\n",
    "    \"NAS_DELAY\",\n",
    "    \"WEATHER_DELAY\",\n",
    "    \"CARRIER_DELAY\"\n",
    "]\n",
    "\n",
    "weird_canceled_flights = weird_canceled_flights.fillna({col: 0 for col in columns_to_fill})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff56995f-4747-432e-ab52-6942d92324f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in weird_canceled_flights: 37486\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 0 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 0 nulls\n",
      "WEATHER_DELAY: 0 nulls\n",
      "NAS_DELAY: 0 nulls\n",
      "SECURITY_DELAY: 0 nulls\n",
      "LATE_AIRCRAFT_DELAY: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = weird_canceled_flights.count()\n",
    "\n",
    "print(\"Number of rows in weird_canceled_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in weird_canceled_flights.columns:\n",
    "    null_count = weird_canceled_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77873ac6-175c-408e-80ed-9f12abe356ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in other_canceled_flights: 935723\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 935723 nulls\n",
      "DEP_DELAY: 935723 nulls\n",
      "TAXI_OUT: 935723 nulls\n",
      "WHEELS_OFF: 935723 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 11 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 935723 nulls\n",
      "WEATHER_DELAY: 935723 nulls\n",
      "NAS_DELAY: 935723 nulls\n",
      "SECURITY_DELAY: 935723 nulls\n",
      "LATE_AIRCRAFT_DELAY: 935723 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = other_canceled_flights.count()\n",
    "\n",
    "print(\"Number of rows in other_canceled_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in other_canceled_flights.columns:\n",
    "    null_count = other_canceled_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c639e25e-f2b2-40d8-aa16-4974f3c71c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill_with_0 = [\n",
    "    \"LATE_AIRCRAFT_DELAY\",\n",
    "    \"SECURITY_DELAY\",\n",
    "    \"NAS_DELAY\",\n",
    "    \"WEATHER_DELAY\",\n",
    "    \"CARRIER_DELAY\"\n",
    "]\n",
    "\n",
    "other_canceled_flights = other_canceled_flights.fillna({col: 0 for col in columns_to_fill_with_0})\n",
    "\n",
    "columns_to_fill_with_minus_1 = [\n",
    "    \"DEP_TIME\",\n",
    "    \"DEP_DELAY\",\n",
    "    \"TAXI_OUT\",\n",
    "    \"WHEELS_OFF\"\n",
    "]\n",
    "\n",
    "other_canceled_flights = other_canceled_flights.fillna({col: -1 for col in columns_to_fill_with_minus_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8005f93b-5333-4630-8562-0737cf18614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling crs elapsed time missing values in other canceled flights\n",
    "from pyspark.sql.functions import floor, col, when\n",
    "\n",
    "# Step 1: Convert CRS_DEP_TIME and CRS_ARR_TIME to total minutes\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_DEP_HOUR\", floor(col(\"CRS_DEP_TIME\") / 100))\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_DEP_MIN\", col(\"CRS_DEP_TIME\") % 100)\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_DEP_TOTAL_MIN\", col(\"CRS_DEP_HOUR\") * 60 + col(\"CRS_DEP_MIN\"))\n",
    "\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_ARR_HOUR\", floor(col(\"CRS_ARR_TIME\") / 100))\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_ARR_MIN\", col(\"CRS_ARR_TIME\") % 100)\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\"CRS_ARR_TOTAL_MIN\", col(\"CRS_ARR_HOUR\") * 60 + col(\"CRS_ARR_MIN\"))\n",
    "\n",
    "# Step 2: Calculate CRS_ELAPSED_TIME\n",
    "other_canceled_flights = other_canceled_flights.withColumn(\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"CRS_ELAPSED_TIME\").isNull(),\n",
    "        when(\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") >= col(\"CRS_DEP_TOTAL_MIN\"),\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"CRS_ARR_TOTAL_MIN\") + 1440) - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        )\n",
    "    ).otherwise(\n",
    "        col(\"CRS_ELAPSED_TIME\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "other_canceled_flights = other_canceled_flights.drop(\"CRS_DEP_HOUR\", \"CRS_DEP_MIN\", \"CRS_DEP_TOTAL_MIN\", \"CRS_ARR_HOUR\", \"CRS_ARR_MIN\", \"CRS_ARR_TOTAL_MIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582c1f67-6890-40d8-ae21-cbba69ba8eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in other_canceled_flights: 935723\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 0 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 0 nulls\n",
      "WEATHER_DELAY: 0 nulls\n",
      "NAS_DELAY: 0 nulls\n",
      "SECURITY_DELAY: 0 nulls\n",
      "LATE_AIRCRAFT_DELAY: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = other_canceled_flights.count()\n",
    "\n",
    "print(\"Number of rows in other_canceled_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in other_canceled_flights.columns:\n",
    "    null_count = other_canceled_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "404f6589-daf6-4f1d-b636-45502492f922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in delayed_flights: 22280599\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 1 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 15262 nulls\n",
      "TAXI_IN: 15261 nulls\n",
      "CRS_ARR_TIME: 2 nulls\n",
      "ARR_TIME: 15261 nulls\n",
      "ARR_DELAY: 75794 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 22280599 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 11 nulls\n",
      "ACTUAL_ELAPSED_TIME: 75077 nulls\n",
      "AIR_TIME: 75076 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 12198707 nulls\n",
      "WEATHER_DELAY: 12198707 nulls\n",
      "NAS_DELAY: 12198707 nulls\n",
      "SECURITY_DELAY: 12198707 nulls\n",
      "LATE_AIRCRAFT_DELAY: 12198707 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = delayed_flights.count()\n",
    "\n",
    "print(\"Number of rows in delayed_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in delayed_flights.columns:\n",
    "    null_count = delayed_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602abe49-9cb6-4ac9-b610-5e89b26456cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling crs dep time null value\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Convert DEP_TIME to total minutes\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"DEP_TOTAL_MIN\",\n",
    "    floor(col(\"DEP_TIME\") / 100) * 60 + (col(\"DEP_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Fill missing CRS_DEP_TIME using DEP_TIME - DEP_DELAY\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"CRS_DEP_TIME\",\n",
    "    when(\n",
    "        col(\"CRS_DEP_TIME\").isNull(),\n",
    "        (floor((col(\"DEP_TOTAL_MIN\") - col(\"DEP_DELAY\")) / 60) * 100.0) + ((col(\"DEP_TOTAL_MIN\") - col(\"DEP_DELAY\")) % 60)\n",
    "    ).otherwise(col(\"CRS_DEP_TIME\"))\n",
    ")\n",
    "\n",
    "# Drop helper column\n",
    "delayed_flights = delayed_flights.drop(\"DEP_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e0cf9d5-e47e-41d0-bec3-4083991e3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "delayed_flights = delayed_flights.filter(\n",
    "    ~(col(\"ARR_TIME\").isNull() & col(\"WHEELS_ON\").isNull() & col(\"TAXI_IN\").isNull())\n",
    ")\n",
    "\n",
    "delayed_flights = delayed_flights.withColumn(\"CANCELLATION_CODE\", lit(\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d32fcdd4-96da-4ba2-a407-8d124b08dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling air time missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Convert WHEELS_OFF and WHEELS_ON to total minutes\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"WHEELS_OFF_MIN\",\n",
    "    floor(col(\"WHEELS_OFF\") / 100) * 60 + (col(\"WHEELS_OFF\") % 100)\n",
    ").withColumn(\n",
    "    \"WHEELS_ON_MIN\",\n",
    "    floor(col(\"WHEELS_ON\") / 100) * 60 + (col(\"WHEELS_ON\") % 100)\n",
    ")\n",
    "\n",
    "# Fill AIR_TIME only where it's null and WHEELS_ON is not null\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"AIR_TIME\",\n",
    "    when(\n",
    "        col(\"AIR_TIME\").isNull() & col(\"WHEELS_ON\").isNotNull(),\n",
    "        when(\n",
    "            col(\"WHEELS_ON_MIN\") >= col(\"WHEELS_OFF_MIN\"),\n",
    "            col(\"WHEELS_ON_MIN\") - col(\"WHEELS_OFF_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"WHEELS_ON_MIN\") + 1440) - col(\"WHEELS_OFF_MIN\")  # handles flights crossing midnight\n",
    "        )\n",
    "    ).otherwise(col(\"AIR_TIME\"))\n",
    ")\n",
    "\n",
    "# Drop temp columns\n",
    "delayed_flights = delayed_flights.drop(\"WHEELS_OFF_MIN\", \"WHEELS_ON_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d714fc-d1aa-428d-90f1-31b8ae395d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling arr delay missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Convert CRS_ARR_TIME and ARR_TIME to total minutes\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"CRS_ARR_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_ARR_TIME\") / 100) * 60 + (col(\"CRS_ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"ARR_TOTAL_MIN\",\n",
    "    floor(col(\"ARR_TIME\") / 100) * 60 + (col(\"ARR_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Fill ARR_DELAY only if it is null and both times are not null\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"ARR_DELAY\",\n",
    "    when(\n",
    "        col(\"ARR_DELAY\").isNull() & col(\"ARR_TIME\").isNotNull() & col(\"CRS_ARR_TIME\").isNotNull(),\n",
    "        when(\n",
    "            col(\"ARR_TOTAL_MIN\") >= col(\"CRS_ARR_TOTAL_MIN\"),\n",
    "            col(\"ARR_TOTAL_MIN\") - col(\"CRS_ARR_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"ARR_TOTAL_MIN\") + 1440) - col(\"CRS_ARR_TOTAL_MIN\")  # handles midnight\n",
    "        )\n",
    "    ).otherwise(col(\"ARR_DELAY\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "delayed_flights = delayed_flights.drop(\"CRS_ARR_TOTAL_MIN\", \"ARR_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92140048-33bf-4826-80db-ff2c14074540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling actual elapsed time missing value\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Step 1: Convert ARR_TIME and DEP_TIME to total minutes\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"ARR_TOTAL_MIN\",\n",
    "    floor(col(\"ARR_TIME\") / 100) * 60 + (col(\"ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"DEP_TOTAL_MIN\",\n",
    "    floor(col(\"DEP_TIME\") / 100) * 60 + (col(\"DEP_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Step 2: Calculate ACTUAL_ELAPSED_TIME only if it is null\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"ACTUAL_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"ACTUAL_ELAPSED_TIME\").isNull() & col(\"ARR_TIME\").isNotNull(),\n",
    "        when(\n",
    "            col(\"ARR_TOTAL_MIN\") >= col(\"DEP_TOTAL_MIN\"),\n",
    "            col(\"ARR_TOTAL_MIN\") - col(\"DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"ARR_TOTAL_MIN\") + 1440) - col(\"DEP_TOTAL_MIN\")  # Handles crossing midnight\n",
    "        )\n",
    "    ).otherwise(col(\"ACTUAL_ELAPSED_TIME\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "delayed_flights = delayed_flights.drop(\"ARR_TOTAL_MIN\", \"DEP_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a090a34-497f-4e63-ae2d-d685c64e5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling crs elapsed time missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Step 1: Convert CRS_ARR_TIME and CRS_DEP_TIME to total minutes\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"CRS_ARR_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_ARR_TIME\") / 100) * 60 + (col(\"CRS_ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"CRS_DEP_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_DEP_TIME\") / 100) * 60 + (col(\"CRS_DEP_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Step 2: Fill CRS_ELAPSED_TIME only if null\n",
    "delayed_flights = delayed_flights.withColumn(\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"CRS_ELAPSED_TIME\").isNull() & col(\"CRS_ARR_TIME\").isNotNull(),\n",
    "        when(\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") >= col(\"CRS_DEP_TOTAL_MIN\"),\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"CRS_ARR_TOTAL_MIN\") + 1440) - col(\"CRS_DEP_TOTAL_MIN\")  # Handles midnight wrap\n",
    "        )\n",
    "    ).otherwise(col(\"CRS_ELAPSED_TIME\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop temp columns\n",
    "delayed_flights = delayed_flights.drop(\"CRS_ARR_TOTAL_MIN\", \"CRS_DEP_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df576f7-7af6-4121-a6a0-122021b93d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill_with_0 = [\n",
    "    \"LATE_AIRCRAFT_DELAY\",\n",
    "    \"SECURITY_DELAY\",\n",
    "    \"NAS_DELAY\",\n",
    "    \"WEATHER_DELAY\",\n",
    "    \"CARRIER_DELAY\"\n",
    "]\n",
    "\n",
    "delayed_flights = delayed_flights.fillna(0, subset=columns_to_fill_with_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aee7146-1b5a-4b1d-9619-7c3af088343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns_to_check = [\n",
    "    \"TAXI_IN\",\n",
    "    \"AIR_TIME\",\n",
    "    \"WHEELS_ON\",\n",
    "    \"ACTUAL_ELAPSED_TIME\",\n",
    "    \"ARR_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"CRS_ELAPSED_TIME\"\n",
    "]\n",
    "\n",
    "# Build a combined filter: keep only rows where all are NOT null\n",
    "delayed_flights = delayed_flights.filter(\n",
    "    ~(\n",
    "        col(\"TAXI_IN\").isNull() |\n",
    "        col(\"AIR_TIME\").isNull() |\n",
    "        col(\"WHEELS_ON\").isNull() |\n",
    "        col(\"ACTUAL_ELAPSED_TIME\").isNull() |\n",
    "        col(\"ARR_TIME\").isNull() |\n",
    "        col(\"CRS_ARR_TIME\").isNull() |\n",
    "        col(\"CRS_ELAPSED_TIME\").isNull()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ffa098-f7b7-48b7-a4be-c5ec848930b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in delayed_flights: 22265335\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 0 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 0 nulls\n",
      "WEATHER_DELAY: 0 nulls\n",
      "NAS_DELAY: 0 nulls\n",
      "SECURITY_DELAY: 0 nulls\n",
      "LATE_AIRCRAFT_DELAY: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = delayed_flights.count()\n",
    "\n",
    "print(\"Number of rows in delayed_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in delayed_flights.columns:\n",
    "    null_count = delayed_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a413be3-fdc9-45e6-a226-0d80eb79f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in non_delayed_flights: 38298412\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 8538 nulls\n",
      "TAXI_IN: 8538 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 8538 nulls\n",
      "ARR_DELAY: 72192 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 38298412 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 12 nulls\n",
      "ACTUAL_ELAPSED_TIME: 70449 nulls\n",
      "AIR_TIME: 70449 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 36989899 nulls\n",
      "WEATHER_DELAY: 36989899 nulls\n",
      "NAS_DELAY: 36989899 nulls\n",
      "SECURITY_DELAY: 36989899 nulls\n",
      "LATE_AIRCRAFT_DELAY: 36989899 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = non_delayed_flights.count()\n",
    "\n",
    "print(\"Number of rows in non_delayed_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in non_delayed_flights.columns:\n",
    "    null_count = non_delayed_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef03571a-5a23-41c7-9cc9-72cca6b54695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "non_delayed_flights = non_delayed_flights.filter(\n",
    "    ~(col(\"ARR_TIME\").isNull() & col(\"WHEELS_ON\").isNull() & col(\"TAXI_IN\").isNull())\n",
    ")\n",
    "\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\"CANCELLATION_CODE\", lit(\"None\"))\n",
    "\n",
    "columns_to_fill_with_0 = [\n",
    "    \"LATE_AIRCRAFT_DELAY\",\n",
    "    \"SECURITY_DELAY\",\n",
    "    \"NAS_DELAY\",\n",
    "    \"WEATHER_DELAY\",\n",
    "    \"CARRIER_DELAY\"\n",
    "]\n",
    "\n",
    "non_delayed_flights = non_delayed_flights.fillna(0, subset=columns_to_fill_with_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ceb164c7-32cc-4bcb-ae0b-d527ed463691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling arr delay missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Convert CRS_ARR_TIME and ARR_TIME to total minutes\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"CRS_ARR_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_ARR_TIME\") / 100) * 60 + (col(\"CRS_ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"ARR_TOTAL_MIN\",\n",
    "    floor(col(\"ARR_TIME\") / 100) * 60 + (col(\"ARR_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Fill ARR_DELAY only if it is null and both times are not null\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"ARR_DELAY\",\n",
    "    when(\n",
    "        col(\"ARR_DELAY\").isNull(),\n",
    "        when(\n",
    "            col(\"ARR_TOTAL_MIN\") >= col(\"CRS_ARR_TOTAL_MIN\"),\n",
    "            col(\"ARR_TOTAL_MIN\") - col(\"CRS_ARR_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"ARR_TOTAL_MIN\") + 1440) - col(\"CRS_ARR_TOTAL_MIN\")  # handles midnight\n",
    "        )\n",
    "    ).otherwise(col(\"ARR_DELAY\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "non_delayed_flights = non_delayed_flights.drop(\"CRS_ARR_TOTAL_MIN\", \"ARR_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42b135a9-2316-4c2b-905b-b73d6c8510db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling crs elapsed time missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Step 1: Convert CRS_ARR_TIME and CRS_DEP_TIME to total minutes\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"CRS_ARR_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_ARR_TIME\") / 100) * 60 + (col(\"CRS_ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"CRS_DEP_TOTAL_MIN\",\n",
    "    floor(col(\"CRS_DEP_TIME\") / 100) * 60 + (col(\"CRS_DEP_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Step 2: Fill CRS_ELAPSED_TIME only if null\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"CRS_ELAPSED_TIME\").isNull(),\n",
    "        when(\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") >= col(\"CRS_DEP_TOTAL_MIN\"),\n",
    "            col(\"CRS_ARR_TOTAL_MIN\") - col(\"CRS_DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"CRS_ARR_TOTAL_MIN\") + 1440) - col(\"CRS_DEP_TOTAL_MIN\")  # Handles midnight wrap\n",
    "        )\n",
    "    ).otherwise(col(\"CRS_ELAPSED_TIME\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop temp columns\n",
    "non_delayed_flights = non_delayed_flights.drop(\"CRS_ARR_TOTAL_MIN\", \"CRS_DEP_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "279177ba-b7fa-4d9f-95b2-5185284ff8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling actual elapsed time missing value\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Step 1: Convert ARR_TIME and DEP_TIME to total minutes\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"ARR_TOTAL_MIN\",\n",
    "    floor(col(\"ARR_TIME\") / 100) * 60 + (col(\"ARR_TIME\") % 100)\n",
    ").withColumn(\n",
    "    \"DEP_TOTAL_MIN\",\n",
    "    floor(col(\"DEP_TIME\") / 100) * 60 + (col(\"DEP_TIME\") % 100)\n",
    ")\n",
    "\n",
    "# Step 2: Calculate ACTUAL_ELAPSED_TIME only if it is null\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"ACTUAL_ELAPSED_TIME\",\n",
    "    when(\n",
    "        col(\"ACTUAL_ELAPSED_TIME\").isNull(),\n",
    "        when(\n",
    "            col(\"ARR_TOTAL_MIN\") >= col(\"DEP_TOTAL_MIN\"),\n",
    "            col(\"ARR_TOTAL_MIN\") - col(\"DEP_TOTAL_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"ARR_TOTAL_MIN\") + 1440) - col(\"DEP_TOTAL_MIN\")  # Handles crossing midnight\n",
    "        )\n",
    "    ).otherwise(col(\"ACTUAL_ELAPSED_TIME\"))\n",
    ")\n",
    "\n",
    "# Step 3: Drop helper columns\n",
    "non_delayed_flights = non_delayed_flights.drop(\"ARR_TOTAL_MIN\", \"DEP_TOTAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3200d602-8ea6-40b8-a40b-7dd821c2f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling air time missing values\n",
    "from pyspark.sql.functions import col, floor, when\n",
    "\n",
    "# Convert WHEELS_OFF and WHEELS_ON to total minutes\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"WHEELS_OFF_MIN\",\n",
    "    floor(col(\"WHEELS_OFF\") / 100) * 60 + (col(\"WHEELS_OFF\") % 100)\n",
    ").withColumn(\n",
    "    \"WHEELS_ON_MIN\",\n",
    "    floor(col(\"WHEELS_ON\") / 100) * 60 + (col(\"WHEELS_ON\") % 100)\n",
    ")\n",
    "\n",
    "# Fill AIR_TIME only where it's null and WHEELS_ON is not null\n",
    "non_delayed_flights = non_delayed_flights.withColumn(\n",
    "    \"AIR_TIME\",\n",
    "    when(\n",
    "        col(\"AIR_TIME\").isNull(),\n",
    "        when(\n",
    "            col(\"WHEELS_ON_MIN\") >= col(\"WHEELS_OFF_MIN\"),\n",
    "            col(\"WHEELS_ON_MIN\") - col(\"WHEELS_OFF_MIN\")\n",
    "        ).otherwise(\n",
    "            (col(\"WHEELS_ON_MIN\") + 1440) - col(\"WHEELS_OFF_MIN\")  # handles flights crossing midnight\n",
    "        )\n",
    "    ).otherwise(col(\"AIR_TIME\"))\n",
    ")\n",
    "\n",
    "# Drop temp columns\n",
    "non_delayed_flights = non_delayed_flights.drop(\"WHEELS_OFF_MIN\", \"WHEELS_ON_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39e920f8-572d-4a58-b5d2-5d94a1a6ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in non_delayed_flights: 38289874\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 0 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 0 nulls\n",
      "WEATHER_DELAY: 0 nulls\n",
      "NAS_DELAY: 0 nulls\n",
      "SECURITY_DELAY: 0 nulls\n",
      "LATE_AIRCRAFT_DELAY: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = non_delayed_flights.count()\n",
    "\n",
    "print(\"Number of rows in non_delayed_flights:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in non_delayed_flights.columns:\n",
    "    null_count = non_delayed_flights.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b708096f-74fa-4a95-9b66-49d6b2d33488",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = weird_canceled_flights.union(other_canceled_flights).union(delayed_flights).union(non_delayed_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f4ee6d1-286e-475a-8fc1-a0fb8b8a50ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged dataframe: 61528418\n",
      "-----------------------------------------\n",
      "FL_DATE: 0 nulls\n",
      "OP_CARRIER: 0 nulls\n",
      "OP_CARRIER_FL_NUM: 0 nulls\n",
      "ORIGIN: 0 nulls\n",
      "DEST: 0 nulls\n",
      "CRS_DEP_TIME: 0 nulls\n",
      "DEP_TIME: 0 nulls\n",
      "DEP_DELAY: 0 nulls\n",
      "TAXI_OUT: 0 nulls\n",
      "WHEELS_OFF: 0 nulls\n",
      "WHEELS_ON: 0 nulls\n",
      "TAXI_IN: 0 nulls\n",
      "CRS_ARR_TIME: 0 nulls\n",
      "ARR_TIME: 0 nulls\n",
      "ARR_DELAY: 0 nulls\n",
      "CANCELLED: 0 nulls\n",
      "CANCELLATION_CODE: 0 nulls\n",
      "DIVERTED: 0 nulls\n",
      "CRS_ELAPSED_TIME: 0 nulls\n",
      "ACTUAL_ELAPSED_TIME: 0 nulls\n",
      "AIR_TIME: 0 nulls\n",
      "DISTANCE: 0 nulls\n",
      "CARRIER_DELAY: 0 nulls\n",
      "WEATHER_DELAY: 0 nulls\n",
      "NAS_DELAY: 0 nulls\n",
      "SECURITY_DELAY: 0 nulls\n",
      "LATE_AIRCRAFT_DELAY: 0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = merged_df.count()\n",
    "\n",
    "print(\"Number of rows in merged dataframe:\", row_count)\n",
    "print(\"-----------------------------------------\")\n",
    "for column in merged_df.columns:\n",
    "    null_count = merged_df.filter(F.col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "996394ac-592f-4479-a484-620974c6717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.write.option(\"header\", True).csv(\"cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
